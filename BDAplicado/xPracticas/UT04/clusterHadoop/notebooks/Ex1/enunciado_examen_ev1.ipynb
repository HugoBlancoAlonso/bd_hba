{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169b1dba-2965-4f86-9d14-83592937e1e1",
   "metadata": {},
   "source": [
    "# BIG DATA APLICADO - Examen 1ª Evaluación\n",
    "\n",
    "**Instrucciones generales**\n",
    "\n",
    "1.\tTodas las sentencias deben ejecutarse desde la línea de comandos en las celdas que hay después del enunciado. No debes realizar ninguna tarea desde fuera de Jupyter.\n",
    "2.\tPuedes **añadir** todas las celdas que necesites siempre y cuando estén antes del siguiente enunciado.\n",
    "3.\tTodas las celdas **deben estar ejecutadas** y debe visualizarse el resultado de salida.\n",
    "4.\t**No es necesario documentar** las respuestas, simplemente debes hacer lo que se pide en el enunciado.\n",
    "5.\tSi un comando falla, explica la causa del error y cómo lo has solucionado.\n",
    "6.\tDebes entregar tanto el **notebook** (fichero `.ipynb`) como el mismo fichero convertido a **PDF** (es muy probable que si intentas convertirlo en el propio contenedor te falle por no tener instalado `pandoc`, si es así descargalo en formato `.md` o `html` y conviértelo en tu máquina física)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0846a60-c083-4e50-af7d-85d27a788537",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**NOMBRE**:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de87b69-d6ee-43de-83ce-5921d1bd405d",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Uso de HDFS (5.5 puntos de RA1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c77da-2112-4a53-8a49-9a3c4f97ef95",
   "metadata": {},
   "source": [
    "### Gestión básica y estructura (1.5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391df71c-d395-49fb-84ec-1af121ad76a7",
   "metadata": {},
   "source": [
    "**Preparación del entorno**\n",
    "\n",
    "- Crea un archivo local en tu máquina llamado `datos_alumno.txt`.\n",
    "- El contenido del archivo debe ser tu nombre completo y tu DNI, repetido en 10 líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aee2f34-03dc-489c-8ab7-d1f72b936610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/Ex1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd Ex1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38c2f798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting datos_alumno.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile datos_alumno.txt\n",
    "Hugo Blanco Alonso 71477289M\n",
    "Hugo Blanco Alonso 71477289M\n",
    "Hugo Blanco Alonso 71477289M\n",
    "Hugo Blanco Alonso 71477289M\n",
    "Hugo Blanco Alonso 71477289M\n",
    "Hugo Blanco Alonso 71477289M\n",
    "Hugo Blanco Alonso 71477289M\n",
    "Hugo Blanco Alonso 71477289M\n",
    "Hugo Blanco Alonso 71477289M\n",
    "Hugo Blanco Alonso 71477289M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c995f545-472e-4f73-a0a4-33b787cb9bc2",
   "metadata": {},
   "source": [
    "**Creación de directorios en HDFS**\n",
    "\n",
    "- Crea la siguiente estructura de directorios dentro de HDFS:\n",
    "    - `/examen/{tus_iniciales}/entradas`\n",
    "    - `/examen/{tus_iniciales}/salidas`\n",
    "    - `/examen/{tus_iniciales}/logs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "617a3cdd-8a7d-49fe-aa24-380e74d1877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /examen\n",
    "!hdfs dfs -mkdir /examen/hba\n",
    "!hdfs dfs -mkdir /examen/hba/entradas\n",
    "!hdfs dfs -mkdir /examen/hba/salidas\n",
    "!hdfs dfs -mkdir /examen/hba/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e3f78-7ca6-4e30-a026-bf51ee8cec43",
   "metadata": {},
   "source": [
    "**Ingesta de datos**\n",
    "\n",
    "- Sube el archivo local `datos_alumno.txt` al directorio HDFS `/examen/{tus_iniciales}/entradas`\n",
    "- Verifica que el archivo se ha subido correctamente listando el contenido del directorio\n",
    "- Verifica que el archivo se ha subido correctamente listando el contenido del archivo `datos_alumno.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6fa936-626d-4a2d-b81e-f8741c50368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put datos_alumno.txt /examen/hba/entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e779103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   3 root supergroup        290 2025-12-04 09:00 /examen/hba/entradas/datos_alumno.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /examen/hba/entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "003486c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugo Blanco Alonso 71477289M\n",
      "Hugo Blanco Alonso 71477289M\n",
      "Hugo Blanco Alonso 71477289M\n",
      "Hugo Blanco Alonso 71477289M\n",
      "Hugo Blanco Alonso 71477289M\n",
      "Hugo Blanco Alonso 71477289M\n",
      "Hugo Blanco Alonso 71477289M\n",
      "Hugo Blanco Alonso 71477289M\n",
      "Hugo Blanco Alonso 71477289M\n",
      "Hugo Blanco Alonso 71477289M\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /examen/hba/entradas/datos_alumno.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e37ba3-f436-4b23-879b-42afe764555a",
   "metadata": {},
   "source": [
    "### Manipulación y exploración (1.5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bced13-a4e8-4443-8f7c-c27aa0d3cbb5",
   "metadata": {},
   "source": [
    "**Duplicación y renombrado**\n",
    "\n",
    "- Realiza una copia del archivo que acabas de subir (`datos_alumno.txt`) dentro de HDFS y colócala en la carpeta `/examen/{tus_iniciales}/salidas`.\n",
    "- Renombra esta copia en HDFS para que se llame `backup_datos.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0604a361-d2e4-4a7c-a0f4-003d95823389",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cp /examen/hba/entradas/datos_alumno.txt /examen/hba/salidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5092a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mv /examen/hba/salidas/datos_alumno.txt /examen/hba/salidas/backup_datos.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a438034-0de2-4f59-8edb-2e5703c99a24",
   "metadata": {},
   "source": [
    "**Inspección de contenido**\n",
    "\n",
    "- Muestra por consola las últimas 3 líneas del archivo `backup_datos.txt` que reside en HDFS\n",
    "- Muestra el tamaño total (en formato legible para los humanos) del directorio `/examen/{tus_iniciales}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9c98db7-62a7-4b58-926c-4e4f2c65aa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0    /examen/hba/entradas\n",
      "290  870  /examen/hba/logs\n",
      "290  870  /examen/hba/salidas\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -du /examen/hba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f77d34-2620-443e-861b-4fefce665335",
   "metadata": {},
   "source": [
    "**Movimiento de datos**\n",
    "\n",
    "- Mueve el archivo original `/examen/{tus_iniciales}/entradas/datos_alumno.txt` a la carpeta `/examen/{tus_iniciales}/logs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64a2f89d-2cf5-4fcd-be21-66279dd53db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mv /examen/hba/entradas/datos_alumno.txt /examen/hba/logs/datos_alumno.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf2383-ecf5-4e10-a6b1-79b59db2afaf",
   "metadata": {},
   "source": [
    "### Administración avanzada (2.5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76470ab0-8733-44b5-ba6b-5e508a16a763",
   "metadata": {},
   "source": [
    "**Factor de replicación**\n",
    "\n",
    "- Cambia el factor de replicación del archivo `/examen/{tus_iniciales}/salidas/backup_datos.txt` a **1**.\n",
    "- Comprueba que el cambio se ha efectuado correctamente utilizando el comando `fsck` o `ls` con los parámetros adecuados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79e2df60-c142-43fb-85f3-28fca588edcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setrep: Illegal replication, a positive integer expected\n",
      "-setrep: For input string: \"/examen/hba/salidas/backup_datos.txt\"\n",
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile [-n] <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum [-v] <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-concat <target path> <src path> <src path> ...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] [-s] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] [-t <thread count>] [-q <thread pool queue size>] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] [-v] [-x] <path> ...]\n",
      "\t[-expunge [-immediate] [-fs <path>]]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n",
      "\t[-head <file>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] [-s <sleep interval>] <file>]\n",
      "\t[-test -[defswrz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touch [-a] [-m] [-t TIMESTAMP (yyyyMMdd:HHmmss) ] [-c] <path> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are:\n",
      "-conf <configuration file>        specify an application configuration file\n",
      "-D <property=value>               define a value for a given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
      "\n",
      "The general command line syntax is:\n",
      "command [genericOptions] [commandOptions]\n",
      "\n",
      "Usage: hadoop fs [generic options] -setrep [-R] [-w] <rep> <path> ...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -setrep /examen/hba/salidas/backup_datos.txt -w 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51845d15-7916-48fb-aed5-dfdc34fa33ab",
   "metadata": {},
   "source": [
    "**Permisos**\n",
    "\n",
    "- Cambia los permisos del directorio `/examen/{tus_iniciales}/logs` para que solo el propietario tenga permisos de lectura, escritura y ejecución. El resto de los usuarios no debe tener acceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19d40e0a-cb1c-4f81-9343-1c966e8ae87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -chmod 600 /examen/hba/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e86afb0-0e57-497e-a10d-74049dc9b7ff",
   "metadata": {},
   "source": [
    "**Gestión de cuotas**\n",
    "\n",
    "- Asigna una cuota de espacio al directorio `/examen/{tus_iniciales}/entradas` limitada a 1 MB.\n",
    "- Intenta subir un archivo (o varios) que superen en total 1 MB a ese directorio para demostrar que la cuota funciona.\n",
    "- Elimina la cuota de espacio asignada al directorio `/examen/{tus_iniciales}/entradas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47f29762-dd65-4b3e-8b4b-f0584d23a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfsadmin -setQuota 1 /examen/hba/entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c32154f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: The NameSpace quota (directories and files) of directory /examen/hba/entradas is exceeded: quota=1 file count=2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put imdb_movies_clean.csv /examen/hba/entradas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea69529-2528-49d7-a5f5-269182dc3cb4",
   "metadata": {},
   "source": [
    "**Snapshots y recuperación**\n",
    "\n",
    "- Habilita la funcionalidad de snapshots en el directorio `/examen/{tus_iniciales}/salidas`.\n",
    "- Crea un snapshot del directorio `/examen/{tus_iniciales}/salidas` llamado `snap_seguridad_v1`.\n",
    "- Simula un error humano borrando el archivo `/examen/{tus_iniciales}/salidas/backup_datos.txt`.\n",
    "- Recupera el archivo borrado restaurándolo desde el snapshot creado anteriormente.\n",
    "- Comprueba que el archivo vuelve a aparecer en su ubicación original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "325f3d6a-107a-45df-b244-1b237e4192f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowing snapshot on /examen/hba/salidas succeeded\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfsadmin -allowSnapshot /examen/hba/salidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31c8da1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created snapshot /examen/hba/salidas/.snapshot/snap_seguridad_v1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -createSnapshot /examen/hba/salidas snap_seguridad_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc4c5e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /examen/hba/salidas/backup_datos.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /examen/hba/salidas/backup_datos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa925bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cp /examen/hba/salidas/.snapshot/snap_seguridad_v1/backup_datos.txt /examen/hba/salidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd97cda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 root supergroup        290 2025-12-04 09:35 /examen/hba/salidas/backup_datos.txt\n",
      "drwxr-xr-x   - root supergroup          0 2025-12-04 09:33 /examen/hba/salidas/snap_seguridad_v1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /examen/hba/salidas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5d6c4-c629-4b0e-b9c9-c7267ad7e5c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a4eb7-106c-47ab-9390-95dc94bdafab",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Computación distribuida con MapReduce (10 puntos de RA2)\n",
    "\n",
    "Esta parte del examen la vamos a hacer con el *dataset* que puedes encontrar en [https://www.kaggle.com/datasets/ashpalsingh1525/imdb-movies-dataset](https://www.kaggle.com/datasets/ashpalsingh1525/imdb-movies-dataset) y que contiene datos sobre más de 10000 películas de IMDB. El fichero del *dataset* te lo habrá facilitado el profesor junto con el examen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d8f28-bb46-4813-885a-15ebc1e7f332",
   "metadata": {},
   "source": [
    "## Número de películas por género"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306debcd-a30c-4b71-9456-c1adec9aa9fd",
   "metadata": {},
   "source": [
    "**Número de películas de cada género**\n",
    "\n",
    "Queremos saber **cuántas películas hay en cada uno de los géneros**. Ten en cuenta que muchas películas pertenecen a más de un género. Consejo: antes de empezar observa y familiarízate con la estructura de los datos del fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea935c2a-42b3-4cea-9ad2-4b3aa1a512fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `clean_file_bueno.csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put clean_file_bueno.csv /examen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "31afbab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "first = True\n",
    "\n",
    "for line in sys.stdin:\n",
    "    if first:\n",
    "        first = False\n",
    "        continue\n",
    "    line = line.strip().split(\",\")\n",
    "    generos = line[3]\n",
    "    print(f\"{generos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ea77549a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "dic = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\";\")\n",
    "\n",
    "    for gen in line:\n",
    "        if gen != \"\":\n",
    "            if gen in dic:\n",
    "                dic[gen] = dic[gen] + 1\n",
    "            else:\n",
    "                dic[gen] = 1\n",
    "\n",
    "\n",
    "for gen, count in dic.items():\n",
    "    print(f\"{gen}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f9e26002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /examen/salida1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /examen/salida1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e84c747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 10:09:20,295 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper1.py, reducer1.py, /tmp/hadoop-unjar4475644127114847066/] [] /tmp/streamjob5822806030821860297.jar tmpDir=null\n",
      "2025-12-04 10:09:20,848 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.3:8032\n",
      "2025-12-04 10:09:20,962 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.3:8032\n",
      "2025-12-04 10:09:21,118 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764838071815_0006\n",
      "2025-12-04 10:09:21,729 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-04 10:09:21,807 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-04 10:09:21,917 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764838071815_0006\n",
      "2025-12-04 10:09:21,917 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-04 10:09:22,065 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-04 10:09:22,065 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-04 10:09:22,120 INFO impl.YarnClientImpl: Submitted application application_1764838071815_0006\n",
      "2025-12-04 10:09:22,151 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764838071815_0006/\n",
      "2025-12-04 10:09:22,152 INFO mapreduce.Job: Running job: job_1764838071815_0006\n",
      "2025-12-04 10:09:27,225 INFO mapreduce.Job: Job job_1764838071815_0006 running in uber mode : false\n",
      "2025-12-04 10:09:27,226 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-04 10:09:31,312 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-04 10:09:37,346 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-04 10:09:37,367 INFO mapreduce.Job: Job job_1764838071815_0006 completed successfully\n",
      "2025-12-04 10:09:37,428 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=241190\n",
      "\t\tFILE: Number of bytes written=1424757\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6626906\n",
      "\t\tHDFS: Number of bytes written=282\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4155\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2695\n",
      "\t\tTotal time spent by all map tasks (ms)=4155\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2695\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4155\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2695\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4254720\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2759680\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10179\n",
      "\t\tMap output records=10177\n",
      "\t\tMap output bytes=220830\n",
      "\t\tMap output materialized bytes=241196\n",
      "\t\tInput split bytes=200\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2304\n",
      "\t\tReduce shuffle bytes=241196\n",
      "\t\tReduce input records=10177\n",
      "\t\tReduce output records=19\n",
      "\t\tSpilled Records=20354\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=153\n",
      "\t\tCPU time spent (ms)=2950\n",
      "\t\tPhysical memory (bytes) snapshot=885399552\n",
      "\t\tVirtual memory (bytes) snapshot=7804772352\n",
      "\t\tTotal committed heap usage (bytes)=709361664\n",
      "\t\tPeak Map Physical memory (bytes)=331673600\n",
      "\t\tPeak Map Virtual memory (bytes)=2600722432\n",
      "\t\tPeak Reduce Physical memory (bytes)=225320960\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2606411776\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6626706\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=282\n",
      "2025-12-04 10:09:37,428 INFO streaming.StreamJob: Output directory: /examen/salida1\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar \\\n",
    "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-file mapper1.py \\\n",
    "-file reducer1.py \\\n",
    "-mapper mapper1.py \\\n",
    "-reducer reducer1.py \\\n",
    "-input /examen/clean_file_bueno.csv \\\n",
    "-output /examen/salida1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a85d4949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 2752\t\n",
      "Adventure: 1890\t\n",
      "Animation: 1468\t\n",
      "Comedy: 2942\t\n",
      "Family: 1407\t\n",
      "Science Fiction: 1261\t\n",
      "Romance: 1575\t\n",
      "Crime: 1272\t\n",
      "Mystery: 862\t\n",
      "Drama: 3812\t\n",
      "Fantasy: 1382\t\n",
      "Horror: 1554\t\n",
      "Thriller: 2605\t\n",
      "War: 282\t\n",
      "Western: 131\t\n",
      "TV Movie: 212\t\n",
      "History: 422\t\n",
      "Documentary: 217\t\n",
      "Music: 277\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /examen/salida1/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5ef6d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat clean_file_bueno.csv | python3 mapper1.py | sort | python3 reducer1.py  > ficht.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33312c3-2947-4992-b100-40ba775d18b8",
   "metadata": {},
   "source": [
    "**Género más popular**\n",
    "\n",
    "Utilizando MapReduce, averigua cuál es el género más popular. Debes utilizar un segundo proceso MapReduce para procesar la salida del anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "46639124-a8c7-40c2-8945-e889bbffd51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\":\")\n",
    "    gen = line[0]\n",
    "    num = int(line[1])\n",
    "    num = f\"{num:05d}\"\n",
    "    print(f\"{gen}, {num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ab2017d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "maxnum = 0\n",
    "maxgen = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\",\")\n",
    "    gen = line[0]\n",
    "    num = int(line[1])\n",
    "\n",
    "    if num > maxnum:\n",
    "        maxnum = num\n",
    "        maxgen = gen\n",
    "\n",
    "print(f\"{maxgen}: {maxnum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5721ae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action, 02752\n",
      "Adventure, 01890\n",
      "Animation, 01468\n",
      "Comedy, 02943\n",
      "Family, 01407\n",
      "Science Fiction, 01261\n",
      "Romance, 01576\n",
      "Crime, 01272\n",
      "Mystery, 00862\n",
      "Drama, 03812\n",
      "Fantasy, 01382\n",
      "Horror, 01554\n",
      "Thriller, 02605\n",
      "War, 00282\n",
      "Western, 00131\n",
      "TV Movie, 00212\n",
      "History, 00422\n",
      "Documentary, 00217\n",
      "Music, 00277\n"
     ]
    }
   ],
   "source": [
    "!cat ficht.txt | python3 mapper2.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fa8d096d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/examen/salida2': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /examen/salida2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2569d2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04 10:03:54,491 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper2.py, reducer2.py, /tmp/hadoop-unjar1729522063239996636/] [] /tmp/streamjob7763788791237173300.jar tmpDir=null\n",
      "2025-12-04 10:03:55,064 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.3:8032\n",
      "2025-12-04 10:03:55,178 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.3:8032\n",
      "2025-12-04 10:03:55,343 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764838071815_0005\n",
      "2025-12-04 10:03:55,706 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-04 10:03:55,817 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-04 10:03:55,934 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764838071815_0005\n",
      "2025-12-04 10:03:55,934 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-04 10:03:56,108 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-04 10:03:56,108 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-04 10:03:56,179 INFO impl.YarnClientImpl: Submitted application application_1764838071815_0005\n",
      "2025-12-04 10:03:56,225 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764838071815_0005/\n",
      "2025-12-04 10:03:56,226 INFO mapreduce.Job: Running job: job_1764838071815_0005\n",
      "2025-12-04 10:04:01,313 INFO mapreduce.Job: Job job_1764838071815_0005 running in uber mode : false\n",
      "2025-12-04 10:04:01,314 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-04 10:04:06,379 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-04 10:04:11,404 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-04 10:04:11,410 INFO mapreduce.Job: Job job_1764838071815_0005 completed successfully\n",
      "2025-12-04 10:04:11,459 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=352\n",
      "\t\tFILE: Number of bytes written=943075\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=619\n",
      "\t\tHDFS: Number of bytes written=13\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5962\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1798\n",
      "\t\tTotal time spent by all map tasks (ms)=5962\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1798\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5962\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1798\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6105088\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1841152\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=19\n",
      "\t\tMap output records=19\n",
      "\t\tMap output bytes=308\n",
      "\t\tMap output materialized bytes=358\n",
      "\t\tInput split bytes=196\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=19\n",
      "\t\tReduce shuffle bytes=358\n",
      "\t\tReduce input records=19\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=38\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=149\n",
      "\t\tCPU time spent (ms)=1950\n",
      "\t\tPhysical memory (bytes) snapshot=934248448\n",
      "\t\tVirtual memory (bytes) snapshot=7804428288\n",
      "\t\tTotal committed heap usage (bytes)=716701696\n",
      "\t\tPeak Map Physical memory (bytes)=348610560\n",
      "\t\tPeak Map Virtual memory (bytes)=2600656896\n",
      "\t\tPeak Reduce Physical memory (bytes)=241098752\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2606125056\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=423\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=13\n",
      "2025-12-04 10:04:11,459 INFO streaming.StreamJob: Output directory: /examen/salida2\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar \\\n",
    "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-file mapper2.py \\\n",
    "-file reducer2.py \\\n",
    "-mapper mapper2.py \\\n",
    "-reducer reducer2.py \\\n",
    "-input /examen/salida1/part-00000 \\\n",
    "-output /examen/salida2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b5157fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drama: 3812\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /examen/salida2/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95790afb-2bbd-485f-80d4-83037bb9084b",
   "metadata": {},
   "source": [
    "**País con películas más rentables**\n",
    "\n",
    "Queremos saber qué país tiene una filmografía más rentable (ten en cuenta que *budget*=presupuesto, *revenue*=ingresos), así que tienes que obtener un listado de países y beneficios promedio por película ((total ingresos - total presupuestos) / número películas de ese país)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba39163-386d-4cd7-8688-34dc078f3eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "first = True\n",
    "\n",
    "for line in sys.stdin:\n",
    "    if first:\n",
    "        first = False\n",
    "        continue\n",
    "    line = line.strip().split(\",\")\n",
    "    presu = float(line[9])\n",
    "    ing = float(line[10])\n",
    "    pais = line[11]\n",
    "    print(f\"{pais},{presu},{ing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ba9533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "dicTotal = {}\n",
    "dicP = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\",\")\n",
    "    pais = line[0]\n",
    "    presu = float(line[1])\n",
    "    ing = float(line[2])\n",
    "\n",
    "    ben = ing - presu\n",
    "\n",
    "    if pais in dicP:\n",
    "        dicP[pais] = dicP[pais] + 1\n",
    "    else:\n",
    "        dicP[pais] = 1\n",
    "\n",
    "    if pais in dicTotal:\n",
    "        dicTotal[pais] = dicTotal[pais] + ben\n",
    "    else:\n",
    "        dicTotal[pais] = ben\n",
    "\n",
    "    for pais, count in dicP.items():\n",
    "\n",
    "        if pais in dicTotal:\n",
    "           dicTotal[pais] =  dicTotal[pais] / count\n",
    "\n",
    "for pais, ben in dicTotal.items():\n",
    "    print(f\"{pais}: {ben}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7b977a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /examen/salida3\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /examen/salida3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f21825eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 11:33:48,509 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper3.py, reducer3.py, /tmp/hadoop-unjar8349237899437099575/] [] /tmp/streamjob2682814036862841793.jar tmpDir=null\n",
      "2025-12-10 11:33:49,023 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.6:8032\n",
      "2025-12-10 11:33:49,127 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.6:8032\n",
      "2025-12-10 11:33:49,285 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1765365313866_0004\n",
      "2025-12-10 11:33:49,568 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-10 11:33:49,631 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-10 11:33:49,727 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1765365313866_0004\n",
      "2025-12-10 11:33:49,727 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-10 11:33:49,868 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-10 11:33:49,868 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-10 11:33:49,932 INFO impl.YarnClientImpl: Submitted application application_1765365313866_0004\n",
      "2025-12-10 11:33:49,964 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1765365313866_0004/\n",
      "2025-12-10 11:33:49,965 INFO mapreduce.Job: Running job: job_1765365313866_0004\n",
      "2025-12-10 11:33:55,044 INFO mapreduce.Job: Job job_1765365313866_0004 running in uber mode : false\n",
      "2025-12-10 11:33:55,045 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-10 11:33:58,091 INFO mapreduce.Job: Task Id : attempt_1765365313866_0004_m_000001_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:129)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:466)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:350)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "\n",
      "2025-12-10 11:33:58,102 INFO mapreduce.Job: Task Id : attempt_1765365313866_0004_m_000000_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:129)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:466)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:350)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "\n",
      "2025-12-10 11:34:02,130 INFO mapreduce.Job: Task Id : attempt_1765365313866_0004_m_000000_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:129)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:466)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:350)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "\n",
      "2025-12-10 11:34:02,132 INFO mapreduce.Job: Task Id : attempt_1765365313866_0004_m_000001_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:129)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:466)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:350)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "\n",
      "2025-12-10 11:34:05,156 INFO mapreduce.Job: Task Id : attempt_1765365313866_0004_m_000000_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:129)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:466)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:350)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "\n",
      "2025-12-10 11:34:06,164 INFO mapreduce.Job: Task Id : attempt_1765365313866_0004_m_000001_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:129)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:466)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:350)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "\n",
      "2025-12-10 11:34:08,174 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-10 11:34:08,181 INFO mapreduce.Job: Job job_1765365313866_0004 failed with state FAILED due to: Task failed task_1765365313866_0004_m_000000\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0 killedMaps:0 killedReduces: 0\n",
      "\n",
      "2025-12-10 11:34:08,226 INFO mapreduce.Job: Counters: 14\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=7\n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tOther local map tasks=6\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14647\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=14647\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14647\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=14998528\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "2025-12-10 11:34:08,226 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar \\\n",
    "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-file mapper3.py \\\n",
    "-file reducer3.py \\\n",
    "-mapper mapper3.py \\\n",
    "-reducer reducer3.py \\\n",
    "-input /examen/clean_file_bueno.csv \\\n",
    "-output /examen/salida3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942998f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: `/examen/salida3/part-00000': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /examen/salida3/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b12a5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: clean_file_bueno.csv: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat clean_file_bueno.csv | python3 mapper3.py | sort | python3 reducer3.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
