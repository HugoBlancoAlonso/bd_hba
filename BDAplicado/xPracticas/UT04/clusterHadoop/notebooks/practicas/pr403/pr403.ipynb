{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e78ff6-fe58-483a-81a2-6ecfab8b4144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/practicas/pr403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd practicas/pr403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "511b091a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/pr403': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /pr403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a363af31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/pr403/logs.log': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put ./logs.log /pr403"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598d8a6",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8a2e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split()\n",
    "    print(f\"{line[8]}, {1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee93906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "dic = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\", \")\n",
    "    num = int(line[0])\n",
    "    count = int(line[1])\n",
    "\n",
    "    if num in dic:\n",
    "        dic[num] = dic[num] + count\n",
    "    else:\n",
    "        dic[num] = count\n",
    "\n",
    "for num, count in dic.items():\n",
    "    print(f\"{num}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af44476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /pr403/salida1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /pr403/salida1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c51ad5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 16:54:58,794 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper1.py, reducer1.py, /tmp/hadoop-unjar4699871641485395965/] [] /tmp/streamjob7467766486674358150.jar tmpDir=null\n",
      "2025-12-03 16:54:59,389 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 16:54:59,495 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 16:54:59,699 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764780814862_0001\n",
      "2025-12-03 16:55:00,837 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-03 16:55:00,923 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-03 16:55:01,041 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764780814862_0001\n",
      "2025-12-03 16:55:01,041 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-03 16:55:01,214 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-03 16:55:01,215 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-03 16:55:01,870 INFO impl.YarnClientImpl: Submitted application application_1764780814862_0001\n",
      "2025-12-03 16:55:01,977 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764780814862_0001/\n",
      "2025-12-03 16:55:01,980 INFO mapreduce.Job: Running job: job_1764780814862_0001\n",
      "2025-12-03 16:55:09,627 INFO mapreduce.Job: Job job_1764780814862_0001 running in uber mode : false\n",
      "2025-12-03 16:55:09,628 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-03 16:55:15,731 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-03 16:55:21,761 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-03 16:55:21,767 INFO mapreduce.Job: Job job_1764780814862_0001 completed successfully\n",
      "2025-12-03 16:55:21,822 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1006\n",
      "\t\tFILE: Number of bytes written=944419\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28591\n",
      "\t\tHDFS: Number of bytes written=62\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8178\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2802\n",
      "\t\tTotal time spent by all map tasks (ms)=8178\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2802\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8178\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2802\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8374272\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2869248\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=800\n",
      "\t\tMap output materialized bytes=1012\n",
      "\t\tInput split bytes=174\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce shuffle bytes=1012\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=144\n",
      "\t\tCPU time spent (ms)=2210\n",
      "\t\tPhysical memory (bytes) snapshot=930402304\n",
      "\t\tVirtual memory (bytes) snapshot=7809896448\n",
      "\t\tTotal committed heap usage (bytes)=710410240\n",
      "\t\tPeak Map Physical memory (bytes)=343158784\n",
      "\t\tPeak Map Virtual memory (bytes)=2602037248\n",
      "\t\tPeak Reduce Physical memory (bytes)=244289536\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2606391296\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=28417\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=62\n",
      "2025-12-03 16:55:21,822 INFO streaming.StreamJob: Output directory: /pr403/salida1\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar \\\n",
    "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-file mapper1.py \\\n",
    "-file reducer1.py \\\n",
    "-mapper mapper1.py \\\n",
    "-reducer reducer1.py \\\n",
    "-input /pr403/logs.log \\\n",
    "-output /pr403/salida1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daee86de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200: 15\t\n",
      "303: 15\t\n",
      "304: 17\t\n",
      "403: 8\t\n",
      "404: 12\t\n",
      "500: 15\t\n",
      "502: 18\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /pr403/salida1/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f18d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200: 15\n",
      "303: 15\n",
      "304: 17\n",
      "403: 8\n",
      "404: 12\n",
      "500: 15\n",
      "502: 18\n"
     ]
    }
   ],
   "source": [
    "!cat logs.log | python3 mapper1.py | sort | python3 reducer1.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a55c0",
   "metadata": {},
   "source": [
    "# Tráfico Total por IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb93c992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split()\n",
    "    ip = line[0]\n",
    "    if line[9] == \"-\":\n",
    "         bytes = 0\n",
    "    else:\n",
    "        bytes = int(line[9])\n",
    "\n",
    "print(f\"{ip}, {bytes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4eb5c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "dic = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\",\")\n",
    "    ip = line[0]\n",
    "    bytes = int(line[1])\n",
    "\n",
    "    if ip in dic:\n",
    "        dic[ip] = dic[ip] + bytes\n",
    "    else:\n",
    "        dic[ip] = bytes\n",
    "\n",
    "for ip, bytes in dic.items(): \n",
    "    print(f\"{ip}: {bytes} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8da4dd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /pr403/salida2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /pr403/salida2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1af1732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 17:10:45,963 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper2.py, reducer2.py, /tmp/hadoop-unjar8038086215335020674/] [] /tmp/streamjob3828672126128387049.jar tmpDir=null\n",
      "2025-12-03 17:10:46,625 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:10:46,746 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:10:46,916 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764780814862_0006\n",
      "2025-12-03 17:10:47,180 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-03 17:10:47,235 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-03 17:10:47,314 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764780814862_0006\n",
      "2025-12-03 17:10:47,314 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-03 17:10:47,437 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-03 17:10:47,437 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-03 17:10:47,489 INFO impl.YarnClientImpl: Submitted application application_1764780814862_0006\n",
      "2025-12-03 17:10:47,520 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764780814862_0006/\n",
      "2025-12-03 17:10:47,521 INFO mapreduce.Job: Running job: job_1764780814862_0006\n",
      "2025-12-03 17:10:51,816 INFO mapreduce.Job: Job job_1764780814862_0006 running in uber mode : false\n",
      "2025-12-03 17:10:51,818 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-03 17:10:55,889 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-03 17:11:00,913 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-03 17:11:00,921 INFO mapreduce.Job: Job job_1764780814862_0006 completed successfully\n",
      "2025-12-03 17:11:00,984 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=51\n",
      "\t\tFILE: Number of bytes written=942509\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28591\n",
      "\t\tHDFS: Number of bytes written=53\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3562\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2315\n",
      "\t\tTotal time spent by all map tasks (ms)=3562\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2315\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3562\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2315\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3647488\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2370560\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=2\n",
      "\t\tMap output bytes=41\n",
      "\t\tMap output materialized bytes=57\n",
      "\t\tInput split bytes=174\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=57\n",
      "\t\tReduce input records=2\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=4\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=120\n",
      "\t\tCPU time spent (ms)=1620\n",
      "\t\tPhysical memory (bytes) snapshot=929218560\n",
      "\t\tVirtual memory (bytes) snapshot=7803904000\n",
      "\t\tTotal committed heap usage (bytes)=717750272\n",
      "\t\tPeak Map Physical memory (bytes)=347271168\n",
      "\t\tPeak Map Virtual memory (bytes)=2601316352\n",
      "\t\tPeak Reduce Physical memory (bytes)=238395392\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2604576768\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=28417\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=53\n",
      "2025-12-03 17:11:00,984 INFO streaming.StreamJob: Output directory: /pr403/salida2\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar \\\n",
    "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-file mapper2.py \\\n",
    "-file reducer2.py \\\n",
    "-mapper mapper2.py \\\n",
    "-reducer reducer2.py \\\n",
    "-input /pr403/logs.log \\\n",
    "-output /pr403/salida2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9e72c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250.68.88.150: 4972 bytes\t\n",
      "46.141.41.90: 4981 bytes\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /pr403/salida2/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48cd99d",
   "metadata": {},
   "source": [
    "# 2. Análisis de comportamiento\n",
    "## URLS más pupulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1773d7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split()\n",
    "    url = line[6]\n",
    "\n",
    "    print(f\"{url}, {1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19314bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "dic = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split()\n",
    "    url = line[0]\n",
    "    count = int(line[1])\n",
    "\n",
    "    if url in dic:\n",
    "        dic[url] = dic[url] + count\n",
    "    else:\n",
    "        dic[url] = count\n",
    "\n",
    "for url, count in dic.items():\n",
    "    print(f\"{url}, {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7634ea57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /pr403/salida3\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /pr403/salida3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c66d5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 17:27:08,021 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper3.py, reducer3.py, /tmp/hadoop-unjar4582899542856014381/] [] /tmp/streamjob9214678586365403708.jar tmpDir=null\n",
      "2025-12-03 17:27:08,591 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:27:08,707 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.4:8032\n",
      "2025-12-03 17:27:08,849 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1764780814862_0011\n",
      "2025-12-03 17:27:09,167 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-03 17:27:09,258 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-03 17:27:09,350 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1764780814862_0011\n",
      "2025-12-03 17:27:09,350 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-03 17:27:09,493 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-03 17:27:09,493 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-03 17:27:09,558 INFO impl.YarnClientImpl: Submitted application application_1764780814862_0011\n",
      "2025-12-03 17:27:09,592 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1764780814862_0011/\n",
      "2025-12-03 17:27:09,593 INFO mapreduce.Job: Running job: job_1764780814862_0011\n",
      "2025-12-03 17:27:13,664 INFO mapreduce.Job: Job job_1764780814862_0011 running in uber mode : false\n",
      "2025-12-03 17:27:13,666 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-03 17:27:17,732 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-03 17:27:21,757 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-03 17:27:21,764 INFO mapreduce.Job: Job job_1764780814862_0011 completed successfully\n",
      "2025-12-03 17:27:21,822 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1811\n",
      "\t\tFILE: Number of bytes written=946029\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28591\n",
      "\t\tHDFS: Number of bytes written=92\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3574\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1596\n",
      "\t\tTotal time spent by all map tasks (ms)=3574\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1596\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3574\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1596\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3659776\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1634304\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=1605\n",
      "\t\tMap output materialized bytes=1817\n",
      "\t\tInput split bytes=174\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce shuffle bytes=1817\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=5\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=115\n",
      "\t\tCPU time spent (ms)=1620\n",
      "\t\tPhysical memory (bytes) snapshot=934154240\n",
      "\t\tVirtual memory (bytes) snapshot=7807586304\n",
      "\t\tTotal committed heap usage (bytes)=723517440\n",
      "\t\tPeak Map Physical memory (bytes)=346570752\n",
      "\t\tPeak Map Virtual memory (bytes)=2600878080\n",
      "\t\tPeak Reduce Physical memory (bytes)=243449856\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2606280704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=28417\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=92\n",
      "2025-12-03 17:27:21,822 INFO streaming.StreamJob: Output directory: /pr403/salida3\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar \\\n",
    "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-file mapper3.py \\\n",
    "-file reducer3.py \\\n",
    "-mapper mapper3.py \\\n",
    "-reducer reducer3.py \\\n",
    "-input /pr403/logs.log \\\n",
    "-output /pr403/salida3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e7cc5b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr,, 23\t\n",
      "/usr/admin,, 17\t\n",
      "/usr/admin/developer,, 18\t\n",
      "/usr/login,, 21\t\n",
      "/usr/register,, 21\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /pr403/salida3/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ec420",
   "metadata": {},
   "source": [
    "## Distribución por método HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a76b9caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper4.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split()\n",
    "    http = line[5]\n",
    "    print(f\"{http}, {1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a636d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer4.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "dic = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\",\")\n",
    "    http = line[0]\n",
    "    count = int(line[1])\n",
    "\n",
    "    if http in dic:\n",
    "        dic[http] = dic[http] + count\n",
    "    else:\n",
    "        dic[http] = count\n",
    "\n",
    "for http, count in dic.items():\n",
    "    print(f\"{http}, {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22b4beed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /pr403/salida4\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /pr403/salida4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b697fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-06 17:55:37,996 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper4.py, reducer4.py, /tmp/hadoop-unjar5139796246872456713/] [] /tmp/streamjob5726218660135801296.jar tmpDir=null\n",
      "2025-12-06 17:55:38,654 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.3:8032\n",
      "2025-12-06 17:55:38,790 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.3:8032\n",
      "2025-12-06 17:55:38,969 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1765043067955_0002\n",
      "2025-12-06 17:55:39,289 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-06 17:55:39,355 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-06 17:55:39,442 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1765043067955_0002\n",
      "2025-12-06 17:55:39,442 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-06 17:55:39,568 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-06 17:55:39,568 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-06 17:55:39,627 INFO impl.YarnClientImpl: Submitted application application_1765043067955_0002\n",
      "2025-12-06 17:55:39,657 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1765043067955_0002/\n",
      "2025-12-06 17:55:39,658 INFO mapreduce.Job: Running job: job_1765043067955_0002\n",
      "2025-12-06 17:55:44,731 INFO mapreduce.Job: Job job_1765043067955_0002 running in uber mode : false\n",
      "2025-12-06 17:55:44,731 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-06 17:55:54,287 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-06 17:55:58,315 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-06 17:55:58,323 INFO mapreduce.Job: Job job_1765043067955_0002 completed successfully\n",
      "2025-12-06 17:55:58,393 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1199\n",
      "\t\tFILE: Number of bytes written=944805\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28591\n",
      "\t\tHDFS: Number of bytes written=44\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12111\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1702\n",
      "\t\tTotal time spent by all map tasks (ms)=12111\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1702\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12111\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1702\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12401664\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1742848\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=993\n",
      "\t\tMap output materialized bytes=1205\n",
      "\t\tInput split bytes=174\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=1205\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=162\n",
      "\t\tCPU time spent (ms)=3000\n",
      "\t\tPhysical memory (bytes) snapshot=913100800\n",
      "\t\tVirtual memory (bytes) snapshot=7807094784\n",
      "\t\tTotal committed heap usage (bytes)=717225984\n",
      "\t\tPeak Map Physical memory (bytes)=345694208\n",
      "\t\tPeak Map Virtual memory (bytes)=2600259584\n",
      "\t\tPeak Reduce Physical memory (bytes)=242614272\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2608689152\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=28417\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=44\n",
      "2025-12-06 17:55:58,393 INFO streaming.StreamJob: Output directory: /pr403/salida4\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar \\\n",
    "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-file mapper4.py \\\n",
    "-file reducer4.py \\\n",
    "-mapper mapper4.py \\\n",
    "-reducer reducer4.py \\\n",
    "-input /pr403/logs.log \\\n",
    "-output /pr403/salida4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3201bdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DELETE, 23\t\n",
      "\"GET, 25\t\n",
      "\"POST, 24\t\n",
      "\"PUT, 28\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /pr403/salida4/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b0d355",
   "metadata": {},
   "source": [
    "## Analisis de navegadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abe7d254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper5.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split()\n",
    "    navegador = line[11]\n",
    "    print(f\"{navegador}, {1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6194d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer5.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "dic = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\",\")\n",
    "    line = line[0]\n",
    "    if line in dic:\n",
    "        dic[line] = dic[line] + 1\n",
    "    else:\n",
    "        dic[line] = 1\n",
    "\n",
    "for line, count in dic.items():\n",
    "    print(f\"{line}, {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92b020c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /pr403/salida5\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /pr403/salida5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b3d8bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-06 18:08:21,357 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper5.py, reducer5.py, /tmp/hadoop-unjar6973581599727951154/] [] /tmp/streamjob2358296235929841562.jar tmpDir=null\n",
      "2025-12-06 18:08:21,876 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.3:8032\n",
      "2025-12-06 18:08:21,979 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.3:8032\n",
      "2025-12-06 18:08:22,129 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1765043067955_0006\n",
      "2025-12-06 18:08:22,449 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-06 18:08:22,535 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-06 18:08:22,635 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1765043067955_0006\n",
      "2025-12-06 18:08:22,635 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-06 18:08:22,797 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-06 18:08:22,797 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-06 18:08:22,865 INFO impl.YarnClientImpl: Submitted application application_1765043067955_0006\n",
      "2025-12-06 18:08:22,906 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1765043067955_0006/\n",
      "2025-12-06 18:08:22,907 INFO mapreduce.Job: Running job: job_1765043067955_0006\n",
      "2025-12-06 18:08:27,965 INFO mapreduce.Job: Job job_1765043067955_0006 running in uber mode : false\n",
      "2025-12-06 18:08:27,966 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-06 18:08:32,028 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-06 18:08:36,048 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-06 18:08:36,054 INFO mapreduce.Job: Job job_1765043067955_0006 completed successfully\n",
      "2025-12-06 18:08:36,104 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1906\n",
      "\t\tFILE: Number of bytes written=946219\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28591\n",
      "\t\tHDFS: Number of bytes written=19\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3799\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1639\n",
      "\t\tTotal time spent by all map tasks (ms)=3799\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1639\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3799\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1639\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3890176\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1678336\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=1700\n",
      "\t\tMap output materialized bytes=1912\n",
      "\t\tInput split bytes=174\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=1912\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=127\n",
      "\t\tCPU time spent (ms)=1460\n",
      "\t\tPhysical memory (bytes) snapshot=934010880\n",
      "\t\tVirtual memory (bytes) snapshot=7805988864\n",
      "\t\tTotal committed heap usage (bytes)=719847424\n",
      "\t\tPeak Map Physical memory (bytes)=348753920\n",
      "\t\tPeak Map Virtual memory (bytes)=2600890368\n",
      "\t\tPeak Reduce Physical memory (bytes)=240488448\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2605465600\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=28417\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=19\n",
      "2025-12-06 18:08:36,105 INFO streaming.StreamJob: Output directory: /pr403/salida5\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar \\\n",
    "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-file mapper5.py \\\n",
    "-file reducer5.py \\\n",
    "-mapper mapper5.py \\\n",
    "-reducer reducer5.py \\\n",
    "-input /pr403/logs.log \\\n",
    "-output /pr403/salida5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2def1e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Mozilla/5.0, 100\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /pr403/salida5/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0726d5f",
   "metadata": {},
   "source": [
    "# 3. Análisis temporal de sesión\n",
    "## Picos de tráfico por horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cc116a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper6.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper6.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split()\n",
    "    \n",
    "    fecha = line[3]\n",
    "    fecha = fecha.strip().split(\":\")\n",
    "    hora = fecha[1]\n",
    "    print(f\"{hora}, {1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd6d7f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer6.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer6.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "dic = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split(\",\")\n",
    "    hora = line[0]\n",
    "    count = int(line[1])\n",
    "    if hora in dic:\n",
    "        dic[hora] = dic[hora] + count\n",
    "    else:\n",
    "        dic[hora] = count\n",
    "\n",
    "for hora, count in dic.items():\n",
    "    print(f\"{hora}, {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "025adace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /pr403/salida6\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /pr403/salida6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc32a4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-06 18:15:37,885 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper6.py, reducer6.py, /tmp/hadoop-unjar2129959955085644586/] [] /tmp/streamjob1954031514229764434.jar tmpDir=null\n",
      "2025-12-06 18:15:38,424 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.3:8032\n",
      "2025-12-06 18:15:38,537 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.3:8032\n",
      "2025-12-06 18:15:38,684 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1765043067955_0009\n",
      "2025-12-06 18:15:39,034 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-06 18:15:39,136 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-06 18:15:39,289 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1765043067955_0009\n",
      "2025-12-06 18:15:39,290 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-06 18:15:39,476 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-06 18:15:39,477 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-06 18:15:39,549 INFO impl.YarnClientImpl: Submitted application application_1765043067955_0009\n",
      "2025-12-06 18:15:39,594 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1765043067955_0009/\n",
      "2025-12-06 18:15:39,595 INFO mapreduce.Job: Running job: job_1765043067955_0009\n",
      "2025-12-06 18:15:43,678 INFO mapreduce.Job: Job job_1765043067955_0009 running in uber mode : false\n",
      "2025-12-06 18:15:43,679 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-06 18:15:47,757 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-06 18:15:52,792 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-06 18:15:52,801 INFO mapreduce.Job: Job job_1765043067955_0009 completed successfully\n",
      "2025-12-06 18:15:52,855 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=906\n",
      "\t\tFILE: Number of bytes written=944219\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28591\n",
      "\t\tHDFS: Number of bytes written=9\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3843\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1577\n",
      "\t\tTotal time spent by all map tasks (ms)=3843\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1577\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3843\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1577\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3935232\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1614848\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=700\n",
      "\t\tMap output materialized bytes=912\n",
      "\t\tInput split bytes=174\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=912\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=118\n",
      "\t\tCPU time spent (ms)=1500\n",
      "\t\tPhysical memory (bytes) snapshot=914976768\n",
      "\t\tVirtual memory (bytes) snapshot=7803203584\n",
      "\t\tTotal committed heap usage (bytes)=714080256\n",
      "\t\tPeak Map Physical memory (bytes)=344784896\n",
      "\t\tPeak Map Virtual memory (bytes)=2599632896\n",
      "\t\tPeak Reduce Physical memory (bytes)=246267904\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2604134400\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=28417\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9\n",
      "2025-12-06 18:15:52,856 INFO streaming.StreamJob: Output directory: /pr403/salida6\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar \\\n",
    "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-file mapper6.py \\\n",
    "-file reducer6.py \\\n",
    "-mapper mapper6.py \\\n",
    "-reducer reducer6.py \\\n",
    "-input /pr403/logs.log \\\n",
    "-output /pr403/salida6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "13c8100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12, 100\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /pr403/salida6/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f22821",
   "metadata": {},
   "source": [
    "## Tasa de error por endPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "214dcb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper7.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper7.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "codigo = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split()\n",
    "    url = line[6]\n",
    "    cod = int(line[8])\n",
    "\n",
    "    if cod >= 400:\n",
    "        codigo = \"error\"\n",
    "    else:\n",
    "        codigo = \"ok\"\n",
    "\n",
    "    print(f\"{url}, {codigo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "91cfa8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer7.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer7.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "total = 0\n",
    "errores = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    total = total + 1\n",
    "\n",
    "    line = line.strip().split(\",\")\n",
    "    url = line[0]\n",
    "    codigo = str(line[1])\n",
    "\n",
    "    if \"error\" in codigo:\n",
    "        errores = errores + 1\n",
    "\n",
    "print(f\"ERRORES TOTALES: {(errores / total) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8e044039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /pr403/salida7\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /pr403/salida7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a8858275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-06 18:30:19,511 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper7.py, reducer7.py, /tmp/hadoop-unjar255563278939809025/] [] /tmp/streamjob5016497248822100873.jar tmpDir=null\n",
      "2025-12-06 18:30:20,133 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.3:8032\n",
      "2025-12-06 18:30:20,240 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.20.0.3:8032\n",
      "2025-12-06 18:30:20,419 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1765043067955_0017\n",
      "2025-12-06 18:30:20,798 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-12-06 18:30:20,889 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-12-06 18:30:20,990 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1765043067955_0017\n",
      "2025-12-06 18:30:20,990 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-06 18:30:21,130 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-12-06 18:30:21,130 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-12-06 18:30:21,185 INFO impl.YarnClientImpl: Submitted application application_1765043067955_0017\n",
      "2025-12-06 18:30:21,214 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1765043067955_0017/\n",
      "2025-12-06 18:30:21,215 INFO mapreduce.Job: Running job: job_1765043067955_0017\n",
      "2025-12-06 18:30:25,279 INFO mapreduce.Job: Job job_1765043067955_0017 running in uber mode : false\n",
      "2025-12-06 18:30:25,280 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-06 18:30:29,323 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-06 18:30:34,353 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-12-06 18:30:34,360 INFO mapreduce.Job: Job job_1765043067955_0017 completed successfully\n",
      "2025-12-06 18:30:34,439 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2070\n",
      "\t\tFILE: Number of bytes written=946547\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28591\n",
      "\t\tHDFS: Number of bytes written=24\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3990\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1541\n",
      "\t\tTotal time spent by all map tasks (ms)=3990\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1541\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3990\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1541\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4085760\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1577984\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=1864\n",
      "\t\tMap output materialized bytes=2076\n",
      "\t\tInput split bytes=174\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=2076\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=145\n",
      "\t\tCPU time spent (ms)=1670\n",
      "\t\tPhysical memory (bytes) snapshot=910970880\n",
      "\t\tVirtual memory (bytes) snapshot=7808045056\n",
      "\t\tTotal committed heap usage (bytes)=713555968\n",
      "\t\tPeak Map Physical memory (bytes)=347021312\n",
      "\t\tPeak Map Virtual memory (bytes)=2600710144\n",
      "\t\tPeak Reduce Physical memory (bytes)=240898048\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2607497216\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=28417\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=24\n",
      "2025-12-06 18:30:34,439 INFO streaming.StreamJob: Output directory: /pr403/salida7\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar \\\n",
    "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-file mapper7.py \\\n",
    "-file reducer7.py \\\n",
    "-mapper mapper7.py \\\n",
    "-reducer reducer7.py \\\n",
    "-input /pr403/logs.log \\\n",
    "-output /pr403/salida7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "92fdbc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES TOTALES: 53.0%\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /pr403/salida7/part-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
